{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9WEDzYAhjDv"
      },
      "source": [
        "## In class Exercise 4\n",
        "\n",
        "The purpose of this exercise is to practice topic modeling.\n",
        "Please use the text corpus you collected in your last in-class-exercise for this exercise.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due tonight November 1st, 2023 at 11:59 PM.\n",
        "**Late submissions cannot be considered.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grzDMdxcTum_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONVFTFI7hjDy"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "\n",
        "You may refer the code here:\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYnAw6cVhjDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106bbb68-ad82-428a-e44d-8b6c62afcc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.242*\".\" + 0.221*\"document\" + 0.082*\"completes\" + 0.082*\"collection\" + 0.082*\"last\"'),\n",
              " (1,\n",
              "  '0.226*\"second\" + 0.183*\"document\" + 0.158*\"first\" + 0.139*\"?\" + 0.055*\".\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"Your first document goes here.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document or the second?\",\n",
        "    \"The last document completes the collection.\",\n",
        "]\n",
        "\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "texts = [\n",
        "    [word for word in word_tokenize(document) if word.lower() not in stop_words]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine an appropriate range for the number of topics (K)\n",
        "min_topics = 2\n",
        "max_topics = 10\n",
        "\n",
        "# # Suppress warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for num_topics in range(min_topics, max_topics + 1):\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=50, iterations=100)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=\"c_v\")\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "\n",
        "# Select the K with the highest coherence score\n",
        "optimal_k = min_topics + coherence_scores.index(max(coherence_scores))\n",
        "\n",
        "#Train the LDA model with the optimal K\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=optimal_k, id2word=dictionary, passes=50, iterations=100)\n",
        "\n",
        "\n",
        "# Summarize the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "\n",
        "print(topics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3IyNrBJA2Icr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rSHPfszG2ICV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoGcdZHlhjD0"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "\n",
        "You may refer the code here:\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdQHe6PghjD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b103e5b5-cddf-4267-fc78-36842658579c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: document, first, second, goes, last\n",
            "Topic 1: completes, last, collection, document, third\n",
            "Topic 2: goes, first, last, completes, collection\n",
            "Topic 3: third, document, goes, completes, last\n",
            "Topic 4: goes, document, second, completes, last\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"Your first document goes here.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document or the second?\",\n",
        "    \"The last document completes the collection.\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "texts = [\n",
        "    \" \".join([word for word in word_tokenize(document) if word.lower() not in stop_words])\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer and transform the data into a document-term matrix\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Estimate the optimal number of topics (K) based on explained variance\n",
        "explained_variances = []\n",
        "K_range = range(2, min(X.shape) + 1)\n",
        "for K in K_range:\n",
        "    svd = TruncatedSVD(n_components=K)\n",
        "    X_reduced = svd.fit_transform(X)\n",
        "    explained_variances.append(svd.explained_variance_ratio_.sum())\n",
        "\n",
        "optimal_K = K_range[explained_variances.index(max(explained_variances))]\n",
        "\n",
        "# Train the final LSA model with the optimal K\n",
        "svd = TruncatedSVD(n_components=optimal_K)\n",
        "X_reduced = svd.fit_transform(X)\n",
        "\n",
        "# Summarize the topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(svd.components_):\n",
        "    top_words_idx = topic.argsort()[:-6:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBHv9hEbhjD1"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "\n",
        "You may refer the code here:\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC-a_gerhjD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47515d04-be2e-4143-bafd-194af506161a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.226*\"document\" + 0.220*\".\" + 0.129*\"first\" + 0.127*\"goes\" + 0.126*\"second\"')\n",
            "(1, '0.190*\"document\" + 0.111*\".\" + 0.110*\"second\" + 0.107*\"?\" + 0.106*\"completes\"')\n",
            "(2, '0.211*\".\" + 0.207*\"document\" + 0.194*\"third\" + 0.059*\"first\" + 0.057*\"last\"')\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"Your first document goes here.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document or the second?\",\n",
        "    \"The last document completes the collection.\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "texts = [\n",
        "    [word for word in word_tokenize(document) if word.lower() not in stop_words]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine an appropriate range for the number of topics (K)\n",
        "min_topics = 2\n",
        "max_topics = 10\n",
        "\n",
        "coherence_scores = []\n",
        "for num_topics in range(min_topics, max_topics + 1):\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=\"c_v\")\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "# Select the K with the highest coherence score\n",
        "optimal_k = min_topics + coherence_scores.index(max(coherence_scores))\n",
        "\n",
        "# Train the LDA model with the optimal K\n",
        "lda_model = LdaModel(corpus, num_topics=optimal_k, id2word=dictionary)\n",
        "\n",
        "# Summarize the topics\n",
        "topics = lda_model.print_topics(num_words=5)  # Adjust the number of words as needed\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43w7JtahjD2"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "\n",
        "You may refer the code here:\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "URiSoYHghjD2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1210e20-e5f3-46f6-8361-566c07bcb541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.23.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.35.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.17.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (2021.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8fe8a3840075>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mumap_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_neighbors'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mumap_model_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mumap_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BERTopic.__init__() got an unexpected keyword argument 'umap_model_args'"
          ]
        }
      ],
      "source": [
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"Your first document goes here.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document or the second?\",\n",
        "    \"The last document completes the collection.\",\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "texts = [\n",
        "    \" \".join([word for word in word_tokenize(document) if word.lower() not in stop_words])\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# Create a document-term matrix (CountVectorizer is used here)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Generate BERTopic with different numbers of topics to find the optimal K\n",
        "min_topics = 2\n",
        "max_topics = 10\n",
        "best_coherence_score = -1\n",
        "best_num_topics = min_topics\n",
        "\n",
        "for num_topics in range(min_topics, max_topics + 1):\n",
        "    # Reduce the number of dimensions for UMAP to avoid the error\n",
        "    umap_args = {'n_components': 5, 'n_neighbors': 15, 'metric': 'cosine'}\n",
        "\n",
        "    model = BERTopic(embedding_model=\"bert-base-uncased\", nr_topics=num_topics, umap_model_args=umap_args)\n",
        "    topics, _ = model.fit_transform(texts)\n",
        "\n",
        "    coherence_score = model.get_coherence()\n",
        "\n",
        "    if coherence_score > best_coherence_score:\n",
        "        best_coherence_score = coherence_score\n",
        "        best_num_topics = num_topics\n",
        "\n",
        "# Train the BERTopic model with the optimal number of topics\n",
        "model = BERTopic(embedding_model=\"bert-base-uncased\", nr_topics=best_num_topics)\n",
        "topics, _ = model.fit_transform(texts)\n",
        "\n",
        "# Summarize the topics\n",
        "topic_words = model.get_topics()\n",
        "for topic_id, words in topic_words:\n",
        "    print(f\"Topic {topic_id}: {', '.join(words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tQQENUhjD2"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.\n",
        "\n",
        "Follow the guidelines from the essay to enhance your explanation:\n",
        "\n",
        "* Writing logic\n",
        "\n",
        "  Pay attention to how you express your thoughts. For example:\n",
        "\n",
        "  * Weak Writing Logic: “Artificial Intelligence is risky because it is new technology.”\n",
        "\n",
        "  * Strong Writing Logic: “Artificial Intelligence presents ethical risks such as data privacy concerns and algorithmic bias, which necessitate cautious implementation and regulation.”\n",
        "\n",
        "* Topic of sentences\n",
        "\n",
        "  * Focus and Direction: It provides a focus and sets the direction for the paragraph, ensuring that the reader knows what to expect.\n",
        "  * Reader Guidance: It serves as a guidepost for the reader, making it easier to follow the flow of ideas and arguments in the document.\n",
        "  * Support for Thesis: In academic papers, topic sentences help in elaborating or providing evidence for the thesis statement or research question.\n",
        "\n",
        "* Writing flow\n",
        "\n",
        "  * Transition: Smooth and logical transitions between sentences, paragraphs, and sections.\n",
        "  * Rhythm: Variation in sentence length and structure to maintain reader engagement.\n",
        "  * Sequence: The order of points or arguments contributes to a smooth reading experience.\n",
        "  For example:\n",
        "    * Weak Writing Flow: “We studied machine learning algorithms. Ethics are important. Data was collected.”\n",
        "    * Strong Writing Flow: “We initiated our study by focusing on machine learning algorithms. Recognizing the ethical implications, we carefully curated our data set.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wev9TGh1hjD3"
      },
      "outputs": [],
      "source": [
        "Comparing the results generated by the four topic modeling algorithms (Latent Dirichlet Allocation - LDA, Latent Semantic Analysis - LSA, Non-Negative Matrix Factorization - NMF, and BERTopic) provides valuable insights into their respective strengths and weaknesses. To assess which one is better, it's essential to consider various aspects of their performance.\n",
        "\n",
        "LDA (Latent Dirichlet Allocation):\n",
        "\n",
        "Strengths:\n",
        "LDA is a well-established topic modeling technique with proven effectiveness in various text analysis tasks.\n",
        "It provides interpretable topics as word distributions over topics.\n",
        "The number of topics (K) can be controlled, offering flexibility.\n",
        "Weaknesses:\n",
        "LDA assumes a Bag of Words (BoW) representation of documents, which may not capture semantic nuances effectively.\n",
        "Topics are represented as probability distributions, which can be less informative for some applications.\n",
        "LSA (Latent Semantic Analysis):\n",
        "\n",
        "Strengths:\n",
        "LSA identifies latent semantic structures within documents by analyzing word co-occurrences.\n",
        "It can capture synonymy and polysemy effectively, as it reduces dimensionality and extracts latent semantic information.\n",
        "Weaknesses:\n",
        "LSA's effectiveness depends on the quality of the term-document matrix, which can be noisy and may not handle complex linguistic patterns well.\n",
        "Like LDA, LSA provides topics as word distributions.\n",
        "NMF (Non-Negative Matrix Factorization):\n",
        "\n",
        "Strengths:\n",
        "NMF enforces non-negativity constraints on the factorization, making the resulting topics more interpretable.\n",
        "It can be applied to diverse data types, such as text, images, and audio.\n",
        "Topics extracted by NMF are often considered more interpretable than LDA.\n",
        "Weaknesses:\n",
        "The choice of K (number of topics) is a critical hyperparameter, and selecting an appropriate value can be challenging.\n",
        "NMF is sensitive to the initialization of factors and may converge to different solutions.\n",
        "BERTopic:\n",
        "\n",
        "Strengths:\n",
        "BERTopic leverages contextual embeddings from pre-trained BERT models, capturing semantic nuances effectively.\n",
        "It does not require extensive preprocessing, making it suitable for diverse text data.\n",
        "The number of topics can be determined based on coherence scores, which can be more data-driven.\n",
        "Weaknesses:\n",
        "BERTopic may be computationally intensive, especially with large datasets and a large number of topics.\n",
        "It relies on pre-trained language models, which may require significant resources for fine-tuning.\n",
        "The choice of the best topic modeling algorithm depends on the specific task and data. If interpretability and well-established techniques are a priority, LDA or NMF may be suitable. However, if capturing semantic nuances and leveraging pre-trained language models are crucial, BERTopic may provide better results. Additionally, BERTopic's data-driven approach to determine the number of topics makes it attractive for applications where the optimal topic count is not known in advance.\n",
        "\n",
        "In conclusion, the best algorithm among LDA, LSA, NMF, and BERTopic depends on the project's goals, the nature of the data, and the resources available. It's essential to evaluate their performance in the context of the specific task to make an informed choice.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}